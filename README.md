# Реализация метода WARP: On the Benefits of Weight Averaged Rewarded Policies

## Введение

Reinforcement Learning from Human Feedback (RLHF) является фундаментальным методом для настройки языковых моделей на основе отзывов человека. Традиционно для этой задачи используется метод Proximal Policy Optimization (PPO), однако он может быть нестабильным и сложным в реализации. Недавние исследования, такие как **Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs** (https://arxiv.org/pdf/2402.14740v1), показали, что более простой метод REINFORCE может решать некоторые задачи алаймента лучше, чем PPO. 

В еще более новой работе **WARP: On the Benefits of Weight Averaged Rewarded Policies** (https://arxiv.org/pdf/2406.16768) предложена модификация метода REINFORCE с использованием различных техник объединения моделей. В данном документе мы рассмотрим основные идеи этой работы и реализуем предлагаемый метод.

## Основные идеи метода WARP

Метод WARP (Weight Averaged Rewarded Policies) базируется на методе REINFORCE и использует технику усреднения весов моделей для улучшения стабильности и производительности алгоритма. Основные компоненты метода включают:

1. **Основной метод REINFORCE:** базовый метод обучения с подкреплением, который обновляет политику модели на основе градиента вознаграждения.
2. **Усреднение весов:** вместо использования одной политики для обновления, метод WARP усредняет веса нескольких моделей, обученных на различных итерациях.
3. **Стабилизация обучения:** усреднение весов моделей помогает стабилизировать процесс обучения и улучшить общую производительность.
