{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-26T10:05:48.706748Z",
     "start_time": "2024-07-26T10:05:38.696722800Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading readme:   0%|          | 0.00/7.81k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d501d0fdbde541a1aeceaeb4da69222c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 21.0M/21.0M [00:13<00:00, 1.59MB/s]\n",
      "Downloading data: 100%|██████████| 20.5M/20.5M [00:16<00:00, 1.25MB/s]\n",
      "Downloading data: 100%|██████████| 42.0M/42.0M [00:26<00:00, 1.56MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d1247f6a25754eb28617c8d45b0b7d21"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cfd2c23a4d004fe1afeb26d974685b9c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0e01d71a0576444bb1afa0e4bf11c35b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"stanfordnlp/imdb\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-26T10:08:39.473983200Z",
     "start_time": "2024-07-26T10:07:23.423980400Z"
    }
   },
   "id": "cde8bcde43e33323",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pairs: 156250000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.DataFrame(dataset['train'])\n",
    "positive_comments = df[df['label'] == 1]['text'].tolist()\n",
    "negative_comments = df[df['label'] == 0]['text'].tolist()\n",
    "\n",
    "pairs = [(pos, neg) for pos in positive_comments for neg in negative_comments]\n",
    "\n",
    "print(f\"Number of pairs: {len(pairs)}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-26T10:38:05.497838100Z",
     "start_time": "2024-07-26T10:36:01.574194300Z"
    }
   },
   "id": "7875e5cfd0e38f6e",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\PavelAgafonov\\miniconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PavelAgafonov\\miniconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6ce0bda0375e46aba25ed9cb5042ac65"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "10b5469f70a5426ca3dfdcb949e0f9f9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4dfdb2b796ff4d76b1df40acb7715ebf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.16 GiB for an array with shape (156250000,) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mMemoryError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[18], line 10\u001B[0m\n\u001B[0;32m      7\u001B[0m model \u001B[38;5;241m=\u001B[39m DistilBertForSequenceClassification\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdistilbert-base-cased\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m# Создание DataFrame с парами для удобства обработки\u001B[39;00m\n\u001B[1;32m---> 10\u001B[0m pairs_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(pairs, columns\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpositive\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnegative\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# Токенизация пар\u001B[39;00m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtokenize_function\u001B[39m(example):\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\pandas\\core\\frame.py:806\u001B[0m, in \u001B[0;36mDataFrame.__init__\u001B[1;34m(self, data, index, columns, dtype, copy)\u001B[0m\n\u001B[0;32m    804\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m columns \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    805\u001B[0m         columns \u001B[38;5;241m=\u001B[39m ensure_index(columns)\n\u001B[1;32m--> 806\u001B[0m     arrays, columns, index \u001B[38;5;241m=\u001B[39m nested_data_to_arrays(\n\u001B[0;32m    807\u001B[0m         \u001B[38;5;66;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001B[39;00m\n\u001B[0;32m    808\u001B[0m         \u001B[38;5;66;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001B[39;00m\n\u001B[0;32m    809\u001B[0m         data,\n\u001B[0;32m    810\u001B[0m         columns,\n\u001B[0;32m    811\u001B[0m         index,  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[0;32m    812\u001B[0m         dtype,\n\u001B[0;32m    813\u001B[0m     )\n\u001B[0;32m    814\u001B[0m     mgr \u001B[38;5;241m=\u001B[39m arrays_to_mgr(\n\u001B[0;32m    815\u001B[0m         arrays,\n\u001B[0;32m    816\u001B[0m         columns,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    819\u001B[0m         typ\u001B[38;5;241m=\u001B[39mmanager,\n\u001B[0;32m    820\u001B[0m     )\n\u001B[0;32m    821\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:520\u001B[0m, in \u001B[0;36mnested_data_to_arrays\u001B[1;34m(data, columns, index, dtype)\u001B[0m\n\u001B[0;32m    517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_named_tuple(data[\u001B[38;5;241m0\u001B[39m]) \u001B[38;5;129;01mand\u001B[39;00m columns \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    518\u001B[0m     columns \u001B[38;5;241m=\u001B[39m ensure_index(data[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39m_fields)\n\u001B[1;32m--> 520\u001B[0m arrays, columns \u001B[38;5;241m=\u001B[39m to_arrays(data, columns, dtype\u001B[38;5;241m=\u001B[39mdtype)\n\u001B[0;32m    521\u001B[0m columns \u001B[38;5;241m=\u001B[39m ensure_index(columns)\n\u001B[0;32m    523\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m index \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:845\u001B[0m, in \u001B[0;36mto_arrays\u001B[1;34m(data, columns, dtype)\u001B[0m\n\u001B[0;32m    842\u001B[0m     data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mtuple\u001B[39m(x) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m data]\n\u001B[0;32m    843\u001B[0m     arr \u001B[38;5;241m=\u001B[39m _list_to_arrays(data)\n\u001B[1;32m--> 845\u001B[0m content, columns \u001B[38;5;241m=\u001B[39m _finalize_columns_and_data(arr, columns, dtype)\n\u001B[0;32m    846\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m content, columns\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:945\u001B[0m, in \u001B[0;36m_finalize_columns_and_data\u001B[1;34m(content, columns, dtype)\u001B[0m\n\u001B[0;32m    942\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(err) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[0;32m    944\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(contents) \u001B[38;5;129;01mand\u001B[39;00m contents[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m==\u001B[39m np\u001B[38;5;241m.\u001B[39mobject_:\n\u001B[1;32m--> 945\u001B[0m     contents \u001B[38;5;241m=\u001B[39m convert_object_array(contents, dtype\u001B[38;5;241m=\u001B[39mdtype)\n\u001B[0;32m    947\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m contents, columns\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:1068\u001B[0m, in \u001B[0;36mconvert_object_array\u001B[1;34m(content, dtype, dtype_backend, coerce_float)\u001B[0m\n\u001B[0;32m   1064\u001B[0m             arr \u001B[38;5;241m=\u001B[39m maybe_cast_to_datetime(arr, dtype)\n\u001B[0;32m   1066\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m arr\n\u001B[1;32m-> 1068\u001B[0m arrays \u001B[38;5;241m=\u001B[39m [convert(arr) \u001B[38;5;28;01mfor\u001B[39;00m arr \u001B[38;5;129;01min\u001B[39;00m content]\n\u001B[0;32m   1070\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m arrays\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:1068\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m   1064\u001B[0m             arr \u001B[38;5;241m=\u001B[39m maybe_cast_to_datetime(arr, dtype)\n\u001B[0;32m   1066\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m arr\n\u001B[1;32m-> 1068\u001B[0m arrays \u001B[38;5;241m=\u001B[39m [convert(arr) \u001B[38;5;28;01mfor\u001B[39;00m arr \u001B[38;5;129;01min\u001B[39;00m content]\n\u001B[0;32m   1070\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m arrays\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:1030\u001B[0m, in \u001B[0;36mconvert_object_array.<locals>.convert\u001B[1;34m(arr)\u001B[0m\n\u001B[0;32m   1028\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mconvert\u001B[39m(arr):\n\u001B[0;32m   1029\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m dtype \u001B[38;5;241m!=\u001B[39m np\u001B[38;5;241m.\u001B[39mdtype(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mO\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m-> 1030\u001B[0m         arr \u001B[38;5;241m=\u001B[39m lib\u001B[38;5;241m.\u001B[39mmaybe_convert_objects(\n\u001B[0;32m   1031\u001B[0m             arr,\n\u001B[0;32m   1032\u001B[0m             try_float\u001B[38;5;241m=\u001B[39mcoerce_float,\n\u001B[0;32m   1033\u001B[0m             convert_to_nullable_dtype\u001B[38;5;241m=\u001B[39mdtype_backend \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnumpy\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1034\u001B[0m         )\n\u001B[0;32m   1035\u001B[0m         \u001B[38;5;66;03m# Notes on cases that get here 2023-02-15\u001B[39;00m\n\u001B[0;32m   1036\u001B[0m         \u001B[38;5;66;03m# 1) we DO get here when arr is all Timestamps and dtype=None\u001B[39;00m\n\u001B[0;32m   1037\u001B[0m         \u001B[38;5;66;03m# 2) disabling this doesn't break the world, so this must be\u001B[39;00m\n\u001B[0;32m   1038\u001B[0m         \u001B[38;5;66;03m#    getting caught at a higher level\u001B[39;00m\n\u001B[0;32m   1039\u001B[0m         \u001B[38;5;66;03m# 3) passing convert_non_numeric to maybe_convert_objects get this right\u001B[39;00m\n\u001B[0;32m   1040\u001B[0m         \u001B[38;5;66;03m# 4) convert_non_numeric?\u001B[39;00m\n\u001B[0;32m   1042\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m dtype \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32mlib.pyx:2522\u001B[0m, in \u001B[0;36mpandas._libs.lib.maybe_convert_objects\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mMemoryError\u001B[0m: Unable to allocate 1.16 GiB for an array with shape (156250000,) and data type int64"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Загрузка токенайзера и модели\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-cased\")\n",
    "\n",
    "# Создание DataFrame с парами для удобства обработки\n",
    "pairs_df = pd.DataFrame(pairs, columns=[\"positive\", \"negative\"])\n",
    "\n",
    "# Токенизация пар\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example['positive'], example['negative'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "# Токенизация и создание датасета\n",
    "tokenized_pairs = pairs_df.apply(tokenize_function, axis=1)\n",
    "\n",
    "# Разделение на тренировочную и тестовую выборки\n",
    "train_df, test_df = train_test_split(tokenized_pairs, test_size=0.1)\n",
    "\n",
    "# Создание Dataset\n",
    "class PairDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.labels = df['label'].tolist()\n",
    "        self.encodings = df['input_ids'].tolist()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = PairDataset(train_df)\n",
    "test_dataset = PairDataset(test_df)\n",
    "\n",
    "# Настройки обучения\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          \n",
    "    num_train_epochs=3,              \n",
    "    per_device_train_batch_size=16,  \n",
    "    per_device_eval_batch_size=64,   \n",
    "    warmup_steps=500,                \n",
    "    weight_decay=0.01,               \n",
    "    logging_dir='./logs',            \n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Обучение модели\n",
    "trainer = Trainer(\n",
    "    model=model,                         \n",
    "    args=training_args,                  \n",
    "    train_dataset=train_dataset,         \n",
    "    eval_dataset=test_dataset             \n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-26T11:10:18.158110300Z",
     "start_time": "2024-07-26T10:38:21.969202400Z"
    }
   },
   "id": "79f04ce860e28d37",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "I = 2\n",
    "M = 2\n",
    "T = 9000\n",
    "lambda_=1/M\n",
    "mu = 0.01  # EMA update rate\n",
    "beta = 0.1  # KL regularization coefficient\n",
    "eta = 0.3  # LITI update rate for the second iteration, use 0.5 if limited compute budget\n",
    "batch_size = 64\n",
    "learning_rate = 5e-5"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-26T10:08:54.533258800Z",
     "start_time": "2024-07-26T10:08:54.503235600Z"
    }
   },
   "id": "ffa561020a39d368",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PavelAgafonov\\miniconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4e22619500d649ddb0ef1e3626349faa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PavelAgafonov\\miniconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\PavelAgafonov\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7413a591a2224cafa7bb477d4d71c30c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9bfa5a3a055d4078b754469fc2d1e940"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "44d3b150e35243e69e107b0abb8e6ab1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1d9b1b52a45946c2ab526392dc869528"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "39ada131f64b434f8280992069bcbe10"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "55af8bf4da474903980eb70c302d1540"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/465 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a813a2bf0b704b76bc73d72dd94c5cea"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PavelAgafonov\\miniconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\PavelAgafonov\\.cache\\huggingface\\hub\\models--distilbert-base-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/263M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4b91f66d6db4476eb719226e37442e02"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\PavelAgafonov\\miniconda3\\Lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Reward model\n",
    "reward_model_name = \"distilbert-base-cased\"\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(reward_model_name)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-26T10:11:40.089160300Z",
     "start_time": "2024-07-26T10:08:59.408617600Z"
    }
   },
   "id": "456a332a5f31836a",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def reward_fn(output, target):\n",
    "    return torch.nn.functional.mse_loss(output, target)\n",
    "\n",
    "# Create prompt dataset\n",
    "def create_prompt_dataset(dataset, num_samples=100):\n",
    "    prompts = []\n",
    "    for i in range(num_samples):\n",
    "        text = dataset['train'][i]['text']\n",
    "        prompt = tokenizer.encode(text, return_tensors='pt')[:, :15]  # Take first 15 tokens\n",
    "        prompts.append(prompt)\n",
    "    return prompts\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-26T10:13:05.792682700Z",
     "start_time": "2024-07-26T10:13:05.771119200Z"
    }
   },
   "id": "d4060c933b1bd391",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "prompt_dataset = create_prompt_dataset(dataset)\n",
    "data_loader = DataLoader(prompt_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize weights\n",
    "theta_sft = model.state_dict()\n",
    "theta_init = theta_sft"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-26T10:13:08.342801700Z",
     "start_time": "2024-07-26T10:13:08.035667400Z"
    }
   },
   "id": "99dc1a4bd0d7f898",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 11\u001B[0m\n\u001B[0;32m      8\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m# Generate completion\u001B[39;00m\n\u001B[1;32m---> 11\u001B[0m inputs \u001B[38;5;241m=\u001B[39m batch\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     12\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mgenerate(inputs, max_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m30\u001B[39m)\n\u001B[0;32m     14\u001B[0m \u001B[38;5;66;03m# Compute reward\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\torch\\cuda\\__init__.py:293\u001B[0m, in \u001B[0;36m_lazy_init\u001B[1;34m()\u001B[0m\n\u001B[0;32m    288\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    289\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    290\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmultiprocessing, you must use the \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspawn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m start method\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    291\u001B[0m     )\n\u001B[0;32m    292\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(torch\u001B[38;5;241m.\u001B[39m_C, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_cuda_getDeviceCount\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m--> 293\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTorch not compiled with CUDA enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    294\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _cudart \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    295\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[0;32m    296\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    297\u001B[0m     )\n",
      "\u001B[1;31mAssertionError\u001B[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# WARP Algorithm\n",
    "for i in range(I):\n",
    "    for m in range(M):\n",
    "        theta_m = theta_init\n",
    "        theta_m_ema = theta_init\n",
    "        for t in range(T):\n",
    "            for batch in data_loader:\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Generate completion\n",
    "                inputs = batch.to('cuda')\n",
    "                outputs = model.generate(inputs, max_length=30)\n",
    "                \n",
    "                # Compute reward\n",
    "                rewards = reward_model(outputs)\n",
    "                kl_div = torch.nn.functional.kl_div(outputs.log_softmax(dim=-1), inputs.log_softmax(dim=-1), reduction='batchmean')\n",
    "                r_beta = reward_fn(rewards, outputs) - beta * kl_div\n",
    "                \n",
    "                # Update theta_m\n",
    "                loss = -r_beta.mean()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Update theta_m_ema\n",
    "                for param_ema, param in zip(theta_m_ema.values(), model.parameters()):\n",
    "                    param_ema.copy_(param_ema * (1 - mu) + param * mu)\n",
    "        \n",
    "        # Update theta_slerp\n",
    "        if m == 0:\n",
    "            theta_slerp = theta_m_ema\n",
    "        else:\n",
    "            for param_slerp, param_ema in zip(theta_slerp.values(), theta_m_ema.values()):\n",
    "                param_slerp.copy_(param_slerp * (1 - lambda_) + param_ema * lambda_)\n",
    "    \n",
    "    # Update theta_init\n",
    "    for param_init, param_slerp in zip(theta_init.values(), theta_slerp.values()):\n",
    "        param_init.copy_(param_init * (1 - eta) + param_slerp * eta)\n",
    "\n",
    "# Save the final model\n",
    "model.save_pretrained(\"path_to_save_model\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-26T10:28:30.248775100Z",
     "start_time": "2024-07-26T10:28:30.104778700Z"
    }
   },
   "id": "136e7b056ae4e6c2",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e268069ac63197a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
